{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Optimization01.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"hO9tiBAYlXuy"},"source":["# Computational Astrophysics 2021\n","\n","---\n","## Eduard Larrañaga\n","\n","Observatorio Astronómico Nacional\\\n","Facultad de Ciencias\\\n","Universidad Nacional de Colombia\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Djrsi-r1lXu1"},"source":["## Optimization. Linear Fit\n","\n","### About this notebook\n","\n","In this notebook we present the method to define a straight line that fits a dataset.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"uZAo3UawnovA"},"source":["Empirical relationships (e.g., the $M-\\sigma_*$ relation for galaxies)\n","are typically established by taking experimental/observational data\n","and fitting an analytic function to them. \n","\n","In this section, we will\n","introduce the most common curve fitting methods. \n","<center>\n","<img src=\"https://i.ibb.co/F6xm5PY/msigma.png\" alt=\"msigma\" border=\"0\" width=\"400\">\n","</center>\n","\n","Figure from:\n","[J. E. Greene & L. C. Ho, ApJ, 641:L21-L24, (2006)](https://ui.adsabs.harvard.edu/abs/2006ApJ...641L..21G/abstract)"]},{"cell_type":"markdown","metadata":{"id":"4R8Hit6UcfN2"},"source":["---\n","\n","### Some Statistical Concepts of Interest\n","\n","We will begin by defining some important statistical concepts."]},{"cell_type":"markdown","metadata":{"id":"YB7VPrlSnqn-"},"source":["\n","#### Standard Deviation\n","\n","Consider a dataset with $N$ datapoints denoted by $\\{ x_i \\}$. The **standard deviation**, $\\sigma$, is a function that shows how much variation or *dispersion* from the average exists in the dataset. It is defined by\n","\n","\\begin{equation}\n","\\sigma = \\sqrt{\\frac{ \\sum_{i=1} ^N (x_i - \\mu)^2}{N}}\n","\\end{equation}\n","\n","where\n","\n","\\begin{equation}\n","\\mu = \\frac{\\sum_{i=1} ^N x_i}{N}.\n","\\end{equation}\n","\n","A low standard deviation indicates that the data points tend to be very close to the mean (expected value) \n","\n","A high standard deviation indicates that the data points are spread out over a large range of values.\n","\n","**Datapoints with different probabilities**\n","\n","If the data points have different probabilities, for example probability $p_i$ for the point $x_i$, the standard deviation is calculated as\n","\n","\\begin{equation}\n","\\sigma = \\sqrt{\\frac{ \\sum_{i=1} ^N p_i(x_i - \\mu)^2}{N}}\n","\\end{equation}\n","\n","where\n","\n","\\begin{equation}\n","\\mu = \\sum_{i=1} ^N p_i x_i .\n","\\end{equation}\n","\n","**Continuous real-valued data**\n","\n","The standard deviation of a continuous real-valued variable $x$ with probability density function $f(x)$ is\n","\\begin{equation}\n","\\sigma = \\sqrt{\\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x) dx}\n","\\end{equation}\n","\n","where\n","\n","\\begin{equation}\n","\\mu = \\int_{-\\infty} ^{\\infty} x f(x) dx.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"O8gXULPuk9ni"},"source":["#### Variance\n","\n","The variance is the squared standard deviation, \n","\n","\\begin{equation}\n","\\text{var}(x_i) = \\sigma^2 = \\frac{ \\sum_{i=1} ^N (x_i - \\mu)^2}{N}\n","\\end{equation}\n","\n","**Properties of the Variance**\n","\n","- Variance is non-negative, \n","\\begin{equation}\n","\\text{var}(x_i) \\geq 0\n","\\end{equation}\n","\n","- Variance is invariant with respect to changes in the location parameter,\n","\\begin{equation}\n","\\text{var}(x_i + a) =\\text{var}(x_i)\n","\\end{equation}\n","\n","- If values are scaled by a constant, the variance is scaled by the square of that constant,\n","\\begin{equation}\n","\\text{var}(a x_i) =a^2 \\text{var}(x_i)\n","\\end{equation}\n","\n","- If values $x_i$ are statistically independent (i.e. uncorrelated),\n","\\begin{equation}\n","\\text{var}\\left(\\sum_i x_i \\right) = \\sum_i \\text{var}(x_i)\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"qgrmFF_WofOY"},"source":["#### Variance of the Mean\n","\n","From the properties above, it is easy to show that the variance of the mean $\\bar{x} = \\frac{1}{N} \\sum_i x_i$ is\n","\n","\\begin{align}\n","\\text{var} (\\bar{x}) = &\\text{var} \\left( \\frac{1}{N} \\sum_i x_i \\right) \\\\ \n","= &\\frac{1}{N^2} \\text{var} \\left( \\sum_i x_i \\right) \\\\\n","= &\\frac{1}{N^2}   \\sum_i \\text{var} \\left( x_i \\right) \\\\\n","= & \\frac{1}{N^2}   \\sum_i \\sigma^2\\\\\n","= & \\frac{1}{N^2}   N \\sigma^2\\\\\n","= & \\frac{\\sigma^2}{N}.\n","\\end{align}\n","\n","This result implies that the variance of the mean decreases when $N$ increases."]},{"cell_type":"markdown","metadata":{"id":"G3bY5Wtg68Kr"},"source":["#### Covariance\n","\n","Consider a sample with $N$ datapoints with the form $(x_i, y_i)$ with $i=1,2,...,N$. \n","\n","The covariance is a measure of the joint variability of two variables and it is defined as\n","\n","\\begin{equation}\n","\\text{cov} (x,y) = \\frac{1}{N} \\sum_{i=1} ^N (x_i - \\mu_x)(y_i - \\mu_y)\n","\\end{equation}\n","\n","where\n","\n","\\begin{align}\n","\\mu_x =& \\frac{\\sum_{i=1} ^N x_i}{N}\\\\\n","\\mu_y =& \\frac{\\sum_{i=1} ^N y_i}{N}.\n","\\end{align}\n","\n","This expression can be re-written without directly referring to the means as\n","\n","\\begin{equation}\n","\\text{cov} (x,y) = \\frac{1}{N^2} \\sum_{i=1} ^N \\sum_{j=1} ^N \\frac{1}{2}(x_i - x_j)(y_i - y_j) = \\frac{1}{N^2} \\sum_{i} \\sum_{j>1}  (x_i - x_j)(y_i - y_j)\n","\\end{equation}\n","\n","Graphically, the covariance is illustrated as follows\n","\n","<center>\n","<a title=\"Cmglee, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Covariance_trends.svg\"><img width=\"128\" alt=\"Covariance trends\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Covariance_trends.svg/128px-Covariance_trends.svg.png\"></a>\n","</center>\n","\n","The sign of the covariance shows the tendency in the \"linear relationship\" between the variables. However, the magnitude of the covariance is not easy to interpret because it is not normalized and hence, it depends on the magnitudes of the variables. The normalized version of the covariance is called the **(Pearson) correlation coefficient**\n","\n","**Covariance with Itself**\n"," \n"," The covariance in which the two variables are identical, corresponds to the variance,\n","\n","\\begin{equation}\n","\\text{cov}(x,x) = \\frac{1}{N} \\sum_{i=1} ^N (x_i - \\mu_x)(x_i - \\mu_x) = \\frac{1}{N} \\sum_{i=1} ^N (x_i - \\mu_x)^2 = \\sigma_x^2 = \\text{var}(x_i)\n","\\end{equation}\n","\n","**Other Properties of the Covariance**\n","\n","Considering $a$ and $b$ as constants,\n","\n","- $\\text{cov}(x,a) = 0 $\n","- $\\text{cov}(x,y) = \\text{cov}(y,x)$\n","- $\\text{cov}(a x, by) = ab\\text{cov}(x,y)$\n","- $\\text{cov}(x + a, y + b) = \\text{cov}(x,y)$\n"]},{"cell_type":"markdown","metadata":{"id":"6px7okbh3xqM"},"source":["#### Pearson Correlation\n","\n","Correlation describes the strength of the linear association between two variables and is equivalent to the normalized version of the covariance. \n","\n","The correlation coefficient is usually denoted as $R$, $r$ or $\\rho$ and it is defined as\n","\\begin{equation}\n","\\text{corr}(x,y) = r = \\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} \n","\\end{equation}\n","\n","Using the definitions above, we have that\n","\n","\\begin{align}\n","r =  &\\left( \\sqrt{\\frac{ \\sum_{i=1} ^N (x_i - \\mu_x)^2}{N}}\n"," \\right)^{-1} \\left( \\sqrt{\\frac{ \\sum_{i=1} ^N (x_i - \\mu_x)^2}{N}}\n"," \\right)^{-1} \\frac{1}{N} \\sum_{i=1} ^N (x_i - \\mu_x)(y_i - \\mu_y)\\\\\n"," r = &\\frac{\\sum_{i=1} ^N (x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\sum_{i=1} ^N (x_i - \\mu_x)^2}\\sqrt{\\sum_{i=1} ^N (y_i - \\mu_y)^2}}\n","\\end{align}\n","\n","Finally, using the expressions\n","\n","\\begin{align}\n","\\mu_x =& \\frac{\\sum_{i=1} ^N x_i}{N}\\\\\n","\\mu_y =& \\frac{\\sum_{i=1} ^N y_i}{N}.\n","\\end{align}\n","\n","we get\n","\n","\\begin{equation}\n","r = \\frac{N\\sum x_i y_i - \\sum x_i \\sum y_i}{\\sqrt{N\\sum x_i^2 - \\left(\\sum x_i \\right)^2}\\sqrt{N\\sum y_i^2 - \\left(\\sum y_i \\right)^2}}\n","\\end{equation}\n","\n","In the following figure, it is shown some of the values of the correlation coefficient for some datapoints distributions,\n","\n","<center>\n","<img src=\"https://i.stack.imgur.com/VNvWW.png\" alt=\"msigma\" border=\"0\" width=\"600\">\n","</center>\n","\n","**Some Properties of the Correlation Coefficient**\n","\n","- The sign of the correlation coefficient indicates the direction of association. \n","\n","- The correlation coefficient is always between $-1$ (perfect negative linear association) and $+1$ (perfect positive linear association).\n","\n","- The value $r=0$ indicates no lienar relationship.\n","\n","- $\\text{corr}(x,y)=\\text{corr}(y,x)$\n","\n","- The correlation coefficient is **sensitive to outliers**.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZZGb90g4-v0m"},"source":["---\n","## Curve Fitting Criteria\n","\n","$N$ data points $(x_i,y_i)$ <br>\n","We will fit a general function $Y(x,\\{a_j\\})$, depending on M-parameters $\\{a_j\\}$ with $j=1,2,3,...,M$ <br>"]},{"cell_type":"markdown","metadata":{"id":"ZFghPFjq-4kZ"},"source":["\n","\n","### Least squares fit\n","\n","\\begin{equation}\n","\\Delta_i = Y(x_i,\\{a_j\\}) - y_i\\,\\,,\n","\\end{equation}\n","\n","In the *least squares fit*, the goal is to minimize the function\n","\n","\\begin{equation}\n","\\Delta(\\{a_j\\}) = \\sum_{i=1}^N \\Delta_i^2 = \\sum_{i=1}^N \\left( Y(x_i,\\{a_j\\}) - y_i \\right)^2\\,\\,.\n","\\end{equation}\n","\n","The square of the difference is used because negative\n","and positive variations would otherwise partially or fully cancel out, leading to a wrong result. "]},{"cell_type":"markdown","metadata":{"id":"YHP6Z66nlXu1"},"source":["\n","\n","\n","\n","### Data with y-uncertainties\n","\n","Observational data having an estimated error as $y_i \\pm \\sigma_i$\n","\n","Now, the function to minimize is $\\chi^2$, defined as\n","\n","\\begin{equation}\n","\\chi^2 (\\{a_j \\}) = \\sum_{i=1}^N \\left(\\frac{\\Delta_i}{\\sigma_i} \\right)^2 = \n","\\sum_{i=1}^N \\left( \\frac{Y(x_i,\\{a_j\\}) - y_i}{\\sigma_i} \\right)^2\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"XtLSfgMKlXu2"},"source":["---\n","## Linear Regression\n","\n","The simplest curve to fit some data is a stright\n","line (**linear regression**). The function to fit has only two parameters: $a_1$ and $a_2$,\n","\n","\\begin{equation}\n","Y(x, \\{a_1,a_2\\}) = a_1 + a_2 x\\,\\,,\n","\\end{equation}\n","\n","The objective is to determine $a_1$ and $a_2$ such that\n","\n","\\begin{equation}\n","\\chi^2(a_1,a_2) = \\sum_{i=1}^N \\frac{1}{\\sigma_i^2} (a_1 + a_2 x_i - y_i)^2\n","\\end{equation}\n","\n","is minimized. Therefore, differentiating this equation and setting the result to zero gives the equations\n","\n","\\begin{equation}\n","\\begin{aligned}\n","\\frac{\\partial \\chi^2}{\\partial a_1} &= 2 \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}(a_1 + a_2x_i - y_i) = 0\\,\\,,\\\\\n","\\frac{\\partial \\chi^2}{\\partial a_2} &= 2 \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}(a_1 + a_2 x_i - y_i) x_i = 0\\,\\,.\n","\\end{aligned}\n","\\end{equation}\n","\n","This system can be written as\n","\n","\\begin{equation}\n","\\begin{aligned}\n","a_1 S + a_2 \\Sigma x - \\Sigma y &= 0\\,\\,,\\\\\n","a_1\\Sigma x + a_2 \\Sigma x^2 - \\Sigma xy &=0\\,\\,,\n","\\end{aligned}\n","\\end{equation}\n","\n","with\n","\n","\\begin{align}\n","S =& \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\\\\n","\\Sigma x =& \\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2}\\\\\n","\\Sigma y =& \\sum_{i=1}^N \\frac{y_i}{\\sigma_i^2}\\\\\n","\\Sigma x^2 =& \\sum_{i=1}^N \\frac{x_i^2}{\\sigma_i^2}\\\\\n","\\Sigma xy =& \\sum_{i=1}^N \\frac{x_i y_i}{\\sigma_i^2}\\\\\n","\\end{align}\n"]},{"cell_type":"markdown","metadata":{"id":"5Zbqill-lXu2"},"source":["Solving for the two unknowns $a_1$ and $a_2$,\n","\n","\\begin{equation}\n","a_1 = \\frac{\\Sigma y\\Sigma x^2 - \\Sigma x \\Sigma xy}{S\\Sigma x^2 - (\\Sigma x)^2}\n","\\,\\,,\\hspace{2em} a_2 = \\frac{S\\Sigma xy - \\Sigma y \\Sigma x}{S\\Sigma x^2 - (\\Sigma x)^2}\\,\\,.\n","\\label{eq:analysis_chi23}\n","\\end{equation}\n","\n","**Notes**\n","- If all $\\sigma_i$ are identical, they will cancel out of the\n","above equations and $a_1$ and $a_2$ will be independent of them.\n","\n","- If the $\\sigma_i$ are unknown, then one can still use the $\\chi^2$ method and\n","just sets $\\sigma_i = 1$."]},{"cell_type":"markdown","metadata":{"id":"MtcpBZiwlXu2"},"source":["---\n","### Data with x- and y-uncertainties\n","\n","Observational data having an estimated error as $y_i \\pm \\sigma_i$ and $x_i \\pm \\sigma^x_i$\n","\n","The uncertainty in $x_i$ can be incorporated in the $\\chi^2$ fit by relating $\\sigma^x_i$ into an additional error $\\sigma^{\\text{extra}}_i$ in $y_i$. \n","\n","To first order, this can be done by writing\n","\n","\\begin{equation}\n","\\sigma_{i,\\text{extra}} = \\left|\\frac{\\partial y}{\\partial x} \\right|_i \\sigma^x_i\\,\\,,\n","\\end{equation}\n","\n","where one needs an approximation for the slope $\\frac{\\partial y}{\\partial x}$. \n","\n","- If both $\\sigma_i$ and $\\sigma_{i,\\text{extra}}$\n","contribute significantly, one simply adds their squares:\n","$\\sigma_{i,\\text{total}}^2$ = $\\sigma_i^2$ + $\\sigma_{i,\\text{extra}}^2$.\n","\n","- If the error in $x_i$ or $y_i$ is asymmetric about $(x_i,y_i)$ one\n","could weigh by the maximum of the left and right error, or use\n","advanced techniques to incorporate this information."]},{"cell_type":"markdown","metadata":{"id":"QaE0AAEulXu2"},"source":["---\n","### Associated Error to the fit parameters $a_j$\n","\n","Associated error bar  $\\sigma_{a_j}^2$  for the\n","curve fit parameter $a_j$.\n","\n","Using first-order error propagation,\n","we have \n","\n","\\begin{equation}\n","\\sigma^2_{a_j} = \\sum_{i=1}^N \\left(\\frac{\\partial a_j}{\\partial y_i}\n","\\right)^2 \\sigma_i^2\\,\\,,\n","\\end{equation}\n","\n","from which we obtain \n","\n","\\begin{equation}\n","\\sigma_{a_1} = \\sqrt{\\frac{\\Sigma x^2}{S\\Sigma x^2 - (\\Sigma x)^2}}\\,\\,,\n","\\hspace{2em} \\sigma_{a_2} = \\sqrt{\\frac{S}{S\\Sigma x^2 - (\\Sigma x)^2}}\\,\\,.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"xQO_Eb6flXu2"},"source":["---\n","If the data set doesn't have an associated set of error bars, the error\n"," $\\sigma_{a_j} = \\sigma_0$ is estimated from the sample variance of the data,\n"," \n","\\begin{equation}\n","\\sigma_0^2 = \\frac{1}{N-2} \\sum_{i=1}^N \\left(y_i - (a_1 + a_2 x_i) \\right)^2\\,\\,.\n","\\end{equation}\n","\n","The normalization factor $N-2$ of the variance is due to the fact that\n","we have taken out two parameters ($a_1$ and $a_2$) from the\n","data."]},{"cell_type":"markdown","metadata":{"id":"bnToU82DlXu3"},"source":["## Non-Linear Fit\n","\n","Many non-linear fitting problems may be transformed to linear\n","problems by a simple change of variables. \n","<br>\n","<br>\n","\n","**Example**\n","\n","Consider a power law\n","\n","\\begin{equation}\n","Z(t, \\{\\alpha,\\beta\\}) = \\alpha t^{\\beta}\\,\\,.\n","\\end{equation}\n","\n","This may be rewritten as $Y(x, \\{a_1,a_2\\}) = a_1 + a_2 x$ with\n","\n","\\begin{equation}\n","Y = \\log Z\\,\\,,\\hspace{1em} x = \\log t\\,\\,\\hspace{1em} a_1 = \\log\n","\\alpha\\,\\,,\\hspace{1em} a_2 = \\beta\\,\\,.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"td9o8SfAlXu3"},"source":["**Example**\n","\n","Consider an exponential\n","\n","\\begin{equation}\n","Z(t, \\{\\alpha, \\beta\\}) = \\alpha e^{\\beta x}\\,\\,.\n","\\end{equation}\n","\n","This may be rewritten as $Y(x, \\{a_1,a_2\\}) = a_1 + a_2 x$ with\n","\n","\\begin{equation}\n","Y = \\ln Z\\,\\,,\\hspace{1em} a_1 = \\ln\n","\\alpha\\,\\,,\\hspace{1em} a_2 = \\beta\\,\\,.\n","\\end{equation}"]}]}