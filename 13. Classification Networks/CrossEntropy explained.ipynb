{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1XqvZFrCt2RWKDoe-FkeIvoh8Ln9o9TwS","authorship_tag":"ABX9TyN6m1iAWfSJugu+p5oRojv2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<center>\n","<img src=\"https://i.ibb.co/b3T5hkz/logo.png\" alt=\"logo\" border=\"0\" width=600>\n"],"metadata":{"id":"bB2VgAZdSyGU"}},{"cell_type":"markdown","source":["---\n","## 02. Cross-Entropy Explained\n","\n","\n","Eduard LarraÃ±aga (ealarranaga@unal.edu.co)\n","\n","---"],"metadata":{"id":"1sJXZQswS3Vd"}},{"cell_type":"markdown","source":["### Abstract\n","\n","In this notebook we explain the cross-entropy function and ist use as a loss function for a neural network.\n","\n","---"],"metadata":{"id":"tCOJ84FdS9ZM"}},{"cell_type":"markdown","source":["---\n","\n","## The Cross-Entropy\n","\n","Cross-entropy is a function arising from the field of information theory. It is build upon the concept of entropy and is used for calculating the difference between two probability distributions for a given random variable or set of events.\n","\n","To introduce this function, remember that *information* quantifies the number of bits required to encode and/or transmit an event. **Lower probability events have more information, higher probability events have less information**.\n","\n","In information theory is importat the notion of â€œsurpriseâ€ of an event. An event is more surprising the less likely it is, meaning it contains more information. Hence: \n","\n","- **Low probability Event (surprising): More information.**\n","\n","- **Higher Probability Event (unsurprising): Less information.**\n","\n","#### Quantifying Information\n","\n","Information $h(x)$ can be calculated for an event x, given the probability of the event $P(x)$ is defined as\n","\n","\\begin{equation}\n","h(x) = - \\log [P(x)].\n","\\end{equation}\n","\n","From this definition it is easy to check that the information associated with an event $x_1$ with probability of occurrence $P(x_1) =1$ is $h(x_1) = -\\log [1] = 0$, i.e. this event has no information associated.\n","\n","On the other hand, the information probability associated with an event $x_0$ with a low probability of occurrence $P(x_0) \\rightarrow 0$ is $h(x_0) = -\\log [P(x_0)] \\rightarrow \\infty$, i.e. this event has a large amount of information associated.\n","\n","#### Entropy \n","\n","Entropy is defined as the number of bits required to transmit a randomly selected event from a probability distribution. \n","\n","A skewed distribution has a low entropy (low information associated), whereas a distribution where events have equal probability has a larger entropy (high information associated). This fact can be undertood by noting that skewed probability distribution has less â€œsurpriseâ€ and in turn a low entropy because likely events dominate. On the other hand, balanced distribution are more surprising and turn have higher entropy because events are equally likely.\n","\n","- **Skewed Probability Distribution (unsurprising): Low entropy.**\n","\n","- **Balanced Probability Distribution (surprising): High entropy.**\n","\n","Mathematically, entropy can be calculated for a set $X$ of discrete states $x$, with a probability $P(x)$ of occurence, as\n","\n","\\begin{equation}\n","S[P(X)] = -\\sum_{x \\in X} P(x) \\log [P(x)].\n","\\end{equation}\n","\n","Here, the $\\log$ is the base-2 logarithm, meaning that the results are in **bits**  (If the base-e or natural logarithm is used instead, the result will have the units called **nats**).\n","\n","---\n","\n","\n","\n","In order to understand this definition, consider a set of 3 discrete events $X = [x_1, x_2, x_3]$ with probabilities of occurrence $P(X) = [P(x_1), P(x_2), P(x_3)] = [0, 1, 0]$, i.e. event $x_2$ has 100% probability occurrence while $x_1$ and $x_3$ have 0% of probability. Then, the entropy associated with this set is\n","\n","\\begin{align}\n","S[P(X)] = &-\\sum_{x \\in X} P(x) \\log [P(x)] \\\\\n","S[P(X)]= &- P(x_1) \\log [P(x_1)] - P(x_2) \\log [P(x_2)] - P(x_3) \\log [P(x_3)] \\\\\n","S[P(X)] = &-\\log [P(x_2)] \\\\\n","S[P(X)] = &-\\log [1] = 0,\n","\\end{align}\n","\n","i.e. this set has zero entropy (the result is completely determined). \n","\n"],"metadata":{"id":"sLUz0CMlBOXi"}},{"cell_type":"code","source":["import numpy as np\n","\n","def entropy(p):\n","  return -sum(p*np.log2(p))\n","\n","\n","\n","# Probability Distribution\n","P = np.array([0., 1., 0.]) + 1.e-16 # we add this small quantity to avoid the divergence of the logarithm!\n","\n","print(f'The entropy is {entropy(P):.2f}')\n"],"metadata":{"id":"VNQA3G7Vu4tM","executionInfo":{"status":"ok","timestamp":1681938946275,"user_tz":300,"elapsed":198,"user":{"displayName":"Eduard Alexis Larranaga","userId":"04402438389940282602"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"35b68c7e-72ec-468f-d806-5dc35524a4ba"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["The entropy is 0.00\n"]}]},{"cell_type":"markdown","source":["\n","Now consider a set of 3 discrete events $ð‘‹=[ð‘¥1,ð‘¥2,ð‘¥3]$  with probabilities of occurrence  $ð‘ƒ(ð‘‹)=[ð‘ƒ(ð‘¥1),ð‘ƒ(ð‘¥2),ð‘ƒ(ð‘¥3)]=[0.5,0.3,0.2]$. The entropy associated with this probability distribution is\n"],"metadata":{"id":"oRnoE127LAGo"}},{"cell_type":"code","source":["# Probability Distribution\n","P = np.array([0.5, 0.3, 0.2]) \n","\n","print(f'The entropy is {entropy(P):.2f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1D3YqTN6LN22","executionInfo":{"status":"ok","timestamp":1681939041601,"user_tz":300,"elapsed":5,"user":{"displayName":"Eduard Alexis Larranaga","userId":"04402438389940282602"}},"outputId":"0f8df1fa-5683-4b72-e71d-8583f40c0eb9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["The entropy is 1.49\n"]}]},{"cell_type":"markdown","source":["Clearly, the entropy is not zero because the probability distribution does not determine any result completely."],"metadata":{"id":"ZBxviEp8Kv_5"}},{"cell_type":"markdown","source":["#### The Cross-Entropy\n","\n","The definition of entropy for a probaility distribution given above can be generalized ot the concept of **cross-entropy** to calculate the number of bits required to represent or transmit an average event from one distribution compared to another distribution.\n","\n","Consider a **target distribution** or underlying probability distribution $P$ and an **approximation of the target distribution** $Q$. \n","\n","**The cross-entropy of $Q$ from $P$ is the number of additional bits to represent an event using $Q$ instead of $P$.**\n","\n","The Cross-entropy is defined mathematically as\n","\n","\\begin{equation}\n","H(P, Q) = â€“ \\sum _{x \\in X} P(x) \\log [Q(x)],\n","\\end{equation}\n","\n","where $P(x)$ is the probability of the event $x$ in $P$, $Q(x)$ is the probability of event $x$ in $Q$ and $\\log$ is the base-2 logarithm, meaning that the results are in bits  (If the base-e or natural logarithm is used instead, the result will have the units called nats).\n","\n"],"metadata":{"id":"_d5apPDlLYvg"}},{"cell_type":"code","source":["def crossentropy(p,q):\n","  return -sum(p*np.log2(q))"],"metadata":{"id":"4iRc3G6PPlre","executionInfo":{"status":"ok","timestamp":1681939220760,"user_tz":300,"elapsed":4,"user":{"displayName":"Eduard Alexis Larranaga","userId":"04402438389940282602"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["In order to understand this definition, consider a set of 3 discrete events $X = [x_1, x_2, x_3]$. Suppose that the target probability distribution is\n","\n","\\begin{equation}\n","P(X) = [P(x_1), P(x_2), P(x_3)] = [0, 1, 0]\n","\\end{equation}\n","\n","i.e. event $x_2$ has 100% probability of occurrence.\n","Now consider an approximated probability distribution of\n","\\begin{equation}\n","Q(X) = [Q(x_1), Q(x_2), Q(x_3)] = [0.6, 0.2, 0.2].\n","\\end{equation}\n","\n","The cross-entropy for these two probability distributions is"],"metadata":{"id":"fpfPb6iQMrDL"}},{"cell_type":"code","source":["# Target Probability Distribution\n","P = np.array([0., 1., 0.]) \n","\n","# Approximate Probability Distribution\n","Q = np.array([0.6, 0.2, 0.2])\n","\n","print(f'The cross-entropy is {crossentropy(P,Q):.2f}')\n"],"metadata":{"id":"5hIear72ytiq","executionInfo":{"status":"ok","timestamp":1681939231265,"user_tz":300,"elapsed":250,"user":{"displayName":"Eduard Alexis Larranaga","userId":"04402438389940282602"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1c5fe24-cdbb-414a-ea39-90dbfcbdb7dc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The cross-entropy is 2.32\n"]}]},{"cell_type":"markdown","source":["Now consider an better approximated probability distribution,\n","\n","\\begin{equation}\n","Q(X) = [Q(x_1), Q(x_2), Q(x_3)] = [0.3, 0.5, 0.2].\n","\\end{equation}\n","\n","In this case, the cross-entropy  is"],"metadata":{"id":"GU2nxBvWnFUx"}},{"cell_type":"code","source":["# Target Probability Distribution\n","P = np.array([0., 1., 0.]) \n","\n","# Approximate Probability Distribution\n","Q = np.array([0.3, 0.5, 0.2])\n","\n","print(f'The cross-entropy is {crossentropy(P,Q):.2f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wedll6bKnHVJ","executionInfo":{"status":"ok","timestamp":1681939297959,"user_tz":300,"elapsed":206,"user":{"displayName":"Eduard Alexis Larranaga","userId":"04402438389940282602"}},"outputId":"1d5b4de5-1338-46b3-8732-a215d21cfa96"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["The cross-entropy is 1.00\n"]}]},{"cell_type":"markdown","source":["Note that this value indicates that the approximate distribution is a better representation of the target distribution.\n","\n","Finally consider the approximate distribution\n","\n","\\begin{equation}\n","Q(X) = [Q(x_1), Q(x_2), Q(x_3)] = [0.1, 0.8, 0.1],\n","\\end{equation}\n","\n","which gives"],"metadata":{"id":"TouGe16HnXlG"}},{"cell_type":"code","source":["# Target Probability Distribution\n","P = np.array([0., 1., 0.]) \n","\n","# Approximate Probability Distribution\n","Q = np.array([0.1, 0.8, 0.1])\n","\n","print(f'The cross-entropy is {crossentropy(P,Q):.2f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9c81zpAntNZ","executionInfo":{"status":"ok","timestamp":1681939323399,"user_tz":300,"elapsed":222,"user":{"displayName":"Eduard Alexis Larranaga","userId":"04402438389940282602"}},"outputId":"bdfd5b34-1ddf-4a13-c62c-8d8425d034ac"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["The cross-entropy is 0.32\n"]}]},{"cell_type":"markdown","source":["##Â The CategoricalCrossentropy and the SparseCategoricalCrossentropy loss functions in `Keras` \n","\n","The [CategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class) and the [SparseCategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class) loss functions are used to measure the  cost of a classification model.\n","\n","In order to use these function, the algorithm may use and encoding to represent the targets. For example, if one has some categorical targets, they are first represented as integer values:\n","\n","- TargetA ---> 0 \n","- TargetB ---> 1\n","- TargetC ---> 2\n","- TargetD ---> 3\n","...\n","\n","Under this encoding, we can use the **'sparsecategorical_crossentropy'** function which is defined as\n","\n","\\begin{equation}\n","S(w) = -\\sum_{i=1}^N y_i\\log (y_i^p) ,\n","\\end{equation}\n","\n","where $w$ represents the parameters to be adjusted in the optimization procedure.\n","\n","\n","\n","Another representation is obtained by using the **one-hot encoding**, which is based on the use of binary vectors. In this case each integer assigned to the categorical targets is represented as a binary vector, that is all zero values except the index of the integer which is marked with a 1. For example:\n","\n","- TargetA ---> 0  ---> [1 0 0 0]\n","- TargetB ---> 1  ---> [0 1 0 0]\n","- TargetC ---> 2  ---> [0 0 1 0]\n","- TargetD ---> 3  ---> [0 0 0 1]\n","...\n","\n","Under this encoding, we can use the **'categorical_crossentropy'** function which is also defined as before\n","\n","\\begin{equation}\n","S(w) = -\\sum_{i=1}^N y_i \\log (y_i^p) \n","\\end{equation}\n"],"metadata":{"id":"JG8S5WKV_hun"}},{"cell_type":"code","source":[],"metadata":{"id":"IlLCMLDb5YaT"},"execution_count":null,"outputs":[]}]}